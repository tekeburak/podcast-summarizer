{
    "podcast_details": {
        "podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)",
        "episode_title": "Explainable AI for Biology and Medicine with Su-In Lee - #642",
        "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress",
        "episode_transcript": " you All right, everyone. Welcome to another episode of the TwiML AI Podcast. I am your host, Sam Charrington, and today I'm joined by Suin Lee. Suin is a professor at the Paul G. Allen School of Computer Science and Engineering at the University of Washington. Before we get going, be sure to take a moment to hit that subscribe button wherever you're listening to today's show. Suin, welcome to the podcast. Thank you for the introduction. I'm looking forward to digging into our talk. You are an invited speaker at the 2023 ICML workshop on computational biology. And we'll be talking about your talk there, which is really centered around your research into explainable AI, an important topic. But before we jump into that, I'd love to have you share a little bit about your background and how you came to work in the field. Thank you so much. So my lab is currently working on a broad spectrum of a problem, for example, developing explainable AI techniques. So that's a core machine learning. And then we also work on identifying cause and treatment of challenging diseases such as cancer and Alzheimer's disease. So that's computational biology. And then also we develop clinical diagnosis or auditing frameworks for clinical AI. And then you asked about how I got into this field. So I was trained as a machine learning researcher when I was a PhD student. I was working on the problem of dealing with high dimensional data. And then at that time, when I was a PhD student at Stanford, In the field of computational biology, there was something really exciting happened, something called a microarray data. So it's a gene expression data that measures expression levels of 20,000 genes. And I suddenly thought that if machine learning researchers develop a powerful and effective method to identify cause of diseases such as cancer and then therapeutic targets, for those diseases. Then as a machine learning researcher, I can contribute hugely to the science and also medicine. And I just fell in love with this field. So that's how I got into the research at the intersection of computational machine learning and computational biology. After I got a job at the University of Washington that has a very strong medical school, and then I had wonderful colleagues, amazing people who had medical data, electronic health records, and then introduced me to this field of EHR data analysis in various clinical departments, anesthesiology and dermatology, and then emergency medicine. And then I just got really interested into the possibility, the potential that AI researchers or machine learning researchers myself and my students can contribute to medicine. That's how I got into this field of largely three fields. So one is machine learning and AI. And the second is computational biology and then clinical medicine. You probably thought that you had to deal with messy data when you were in clinical biology and computational biology until you saw some of that EHR data. That data can be very messy. It is. The goals of the fields are slightly different to each other. But in the future, I strongly believe that those two fields will merge, biology and medicine. So in a clinical side, researchers are already generating the biological molecular biology data from patients. So for example, for cancer patients, you can think about measuring the gene expression levels or genetic data from those cancer patients. And then what you want is the treatment. You know, you want the AI or machine learning models to tell you, you know, which treatment, which drug, anti-cancer drugs are going to work the best for that particular patient. For that, you definitely need biological knowledge and then, you know, actual mechanistic understanding of cancer. And what says to you that the fields will merge as opposed to kind of collaborate closely? Clearly they need to collaborate closely, but when I think of merge, and maybe I'm taking this too far, I'm thinking of like single models that operate in both domains. Yeah, I know what you're saying. So I tell my students or other young people that to actually move the field forward, to advance this field of biology, medicine, or biomedical sciences, you really need to become a bilingual researcher, or even trilingual these days. You know, computer science plus biology plus medicine. When you are you have one brain that really thinks like, you know, machine learning researchers and biologists and then clinical experts. It's, you know, usually that really helps to come up with creative approach and that can really move the field to benefit patients. And then at the end, the ultimate goal of a biology and molecular biology is to understand life better so that you can advance the health. of humans, right? So I think collaborations definitely help, but at the end, we really need to think about how to produce these young researchers so that they really think like experts in this area. These things already happened earlier in computational biology than clinical medicine. And when I was doing the PhD, it was usually based on collaborations. people who were trained primarily as a machine learning researcher and people who were trained as molecular biologists who hold pipettes and they work in the wet labs and then they form a collaboration and then write papers. But then later, you know, we see a lot of departments that's named, you know, computational biology or, you know, biomedical science departments. So it's a really healthy move for, you know, this kind of interdisciplinary fields. It makes total difference. Yeah. Your research and again, your presentation at the conference are focused on explainable AI, XAI. Tell us a little bit about some of the things that you think are most important about explainability as applied to these fields. I think we get that machine learning and models in general can be opaque and make important high stakes decisions. You need some degree of explainability. What's unique about your take in applying applicability in your field? Right. OK, thank you. That's an excellent question. So the core part of explainable AI, at least this theoretical framework, it basically means feature attributions. So imagine you have a black box model. You have a set of input, a vector x. And then you have an output y. And then when you have a prediction, you want to find a way to attribute two features. You want to know which features contributed the most. And then, you know, there are mathematical frameworks. Our particular approach that's called the SHAP framework, it is based on game theory. So you want to find a way to understand which features are important. So that's the core of the technical side of explainable AI. And then on the other hand, if you just apply this explainable AI technique, you know, off the shelf explainable AI algorithm to biology, mostly it's useless. It's not very useful. It's not useful in terms of biological insights. What you really want to understand is how these features collaborate with each other. Imagine that you have a set of genes as a feature. So you have 20,000 genes, 20,000 expression levels are the input of the black box model. And then your prediction is which cancer drug is going to work the best for each patient. And then individual genes contributions and then gene importance scores by themselves, they are not going to be really useful. It will be only useful when some explainable AI model, explainable AI algorithm can tell you which pathway, how genes collaborate with each other and then how genetic factors play a role into that. And then also how that leads to the good prognosis of the cancer patient and also sensitivity, the good responsiveness to that drug. So there is something missing there. And then the uniqueness of my research is that we want to develop this explainable AI method for biology and then also clinical medicine such that it can make real meaningful contribution to these fields. Another example in the medicine side is that imagine that you have a deep model, deep neural network, that's going to take you a dermatology image. So say that you find something unusual in your skin and then you take a picture. That's your dermatological image. And then let's say that you want to know that has features of melanoma or not. So the prediction results itself is not going to be really useful. And then even the current explainable AI methods that's going to tell you which pixels, which parts of the images led to the prediction of melanoma or not, those are not going to be very useful to understand how this black box model really works. When you try, for example, that you modify the image and then generate a counterfactual, small changes to the image such that it changes the prediction. Let's say that that changes the prediction from melanoma to normal. Only then you can understand how this model works, what the reasoning process of this black box machine learning model is like. So those examples, I'm going to show many examples like that. Basically, the message there is going to be that the current state-of-the-art explainable AI that tells you theoretically supported importance values for the features are not going to be enough to make meaningful contributions to both biological science and then also clinical medicine as well. It sounds like you're calling out a broad deficiency in the approach and kind of saying that as opposed to this feature level explainability, we need more system level or process level explainability that is more grounded in the use cases or the application than what we have available today. Exactly. The question is how to do that. For that, we need a new explainable AI method. In the first part of the talk, I'm going to show many examples of what explainable AI, almost as is, can do. Those are the papers that we published a couple of years ago, so that it addresses new scientific questions. Even explainable AI or feature attribution methods as is can be useful. So I'm going to show many examples like that in both biology and medicine. But in the second part of the talk, I'm going to show how explainable AI can even open new research directions specifically for biology and health care. So those examples I showed you, the systems level insights or this counterfactual image generation that can facilitate collaboration with humans, in this case, a clinical expert. So in the second part of the talk, I'm going to show how this explainably I can open new research directions. And then part of the second part will be I'm going to have a deep dive into our recent paper, to highlight how Explainable AI can help cancer medicine design, cancer therapy design. So basically, how to choose two chemotherapy drugs that's going to have a synergy for a particular patient. So that's the paper that was recently published in Nature Biomedical Engineering. Before we dig into that paper, the most recent paper, can you talk us through in a little bit more detail some of the examples of the foundational machine learning research and how they contribute to the problems you're trying to solve? Okay. So some of the foundational AI methods we developed, I'm going to talk about. It can be summarized into three parts. So one is, you know, principled understanding of current explainable AI methods. So specifically feature attribution methods. So, for example, in one work, we showed that our feature attribution method, that's SHAP, it was published in NeurIPS in 2017, we showed that it unifies a large portion of the explainable AI literature and 25 methods following the exact same principle, and all explaining by removing features. So it turned out that 25 methods, feature attribution methods that are widely used in the field and machine learning applications, they all go by the same principle. You want to assess the importance of each feature by removing them or removing subsets of them. So that helps us understand what goes on. For example, when they fail, you want to understand what goes on and also improve and then develop new explainable AI methods. So I'm going to introduce a couple of unifying frameworks. So this is about how to understand the principled understanding of feature attribution methods. Also, on a computational side, we have explored many avenues to make this SHAP computation even feasible and faster. So, SHAP stands for Shapely Editive... I suddenly forgot. I can't forget this. Explanations. Yes, a sharply additive explanation. It's kind of weird because they chose the third letter of the word. That's the first author, my student, Scott's choice. I love the name, by the way. Computing sharp values is theoretically very well supported, but then computation-wise, it's not really easy to compute. It involves exponential computation. So we need to develop approximation methods such that we can compute them in a feasible manner. So we developed many fast statistical estimation approaches And then you want to make sure that there is a convergence and all the desirable theoretical properties are already there. And then also, we developed approaches for specific model types. For example, ensemble tree models. And then also deep neural networks. So we have a deep shape and then tree shape. And then more recently, we also have a vision transformer Shapley. So that's a way to compute the Shapley values for transformers, vision transformers. And then there is another one that's called the FASTA SHAPE. So the one way to make the SHAPE computation more feasible is to focus on specific particular aspects of models. So for example, tree ensembles or deep neural network, they have some particular model types. There is a way to make this computation a little faster, basically make... So model specific versions of SHAPE implementation. Yes, yes. Yeah. So that's another line of research. And then more recently, we also started to understand the robustness of the shaft value. So adversarial attack. A few years ago, in the field of machine learning, researchers have tried to understand how robust the machine learning model itself, the prediction results are toward adversarial attacks. And then now we are looking into this issue in terms of the model explanations. So how feature attributions are robust. So in our most recent paper, we basically showed the removal-based approaches, including SHAPE. Like earlier I said, many of the feature attribution methods turned out to be to have the same principle, which is explaining by removal. So those line of, you know, methods is more robust to this kind of adversarial attacks. So, and then, you know, multimodality, you know, those other kinds of issues, we are actively doing this research in terms of, you know, foundational AI algorithms also. And SHAP, as you've mentioned, is broadly used, both the original algorithm as well as the related algorithms as you described. But it's also one of the first explainability approaches to be popularized Where does it sit in terms of relevance? Are there different kind of wholly different approaches that have overtaken it in popularity or applicability based on kind of today's models and applications or is SHAP still kind of a core approach to the way explainability is looked at in practice? It's more on the later side. We believe that this removal-based approach and in this cooperative game theory, we believe in that. And then also, it has the desirable properties, first of all. And then we, in our many experiments, we still see that removal-based approaches are more robust, as I said, those berserker attacks. And then also, in terms of various evaluation criteria, we still think that those methods are more robust than the other class, which we characterized as a propagation-based approach or gradient-based approaches. So we would prefer just remover-based approaches. But on the other hand, those approaches are very computationally very intensive. So Well, the way SHEP works is basically that you try all subset of features and then you add a feature of interest and then see the model, check the model output and you average across all subsets of features. So as you can imagine, it's computationally very intensive. So when we now think about foundational models or large language models, these really large models of a ton, a lot of parameters. And then deep neural network and, you know, gradient computation is perhaps easier than trying all sorts of features, right? So practically, it's not, you know, as easy as the other class in terms of the computation, but we still want to make this computational more feasible. We want to develop various clever approaches to reduce the computation and then still maintain the desirable theoretical properties that this removal-based approach or SHAP in particular has. Got it. And so that is an example of kind of the foundational research that your lab does that contributes not only to your work on the biological science side or computational biology side, but broadly to the field. And then your more recent paper is an example of the kind of contributions you're making on the medicine side. Can you talk a little bit about that cancer paper? Yeah, sure. It is about AML. So we chose AML as an example application. So it's acute myeloid leukemia, it's aggressive blood cancer, and it's relatively common for older people. So to give you a bit of a background in general, the cutting edge in the treatment of cancers, such as AML, has increasingly become combination therapy. So the rationale here is that by choosing drugs that target complementary biological pathways, we can achieve greater anti-cancer efficacy. So basically, you choose two or three chemotherapy drugs, and then use them together so that when there is a synergy, usually there is a very good anti-cancer efficacy. But the issue is that choosing optimal combinations of drugs is a really hard problem. So there are about hundreds of individual FDA approved anti-cancer drugs, which means that there will be tens of thousands of possible combinations. But when you consider pairwise combination, and there could be even more if you consider non-FDA approved experimental drugs in development, or consider a combination of more than two drugs. And then the different patients, even patients who have the same type of cancer may respond differently to exact same drugs because of this individual, the particular genomic characteristics. then formulate this problem as a machine learning problem. So you take this AML patient's gene expression levels, so you get the blood of the patient and then purify the cells so you have only cancer cells, and then say you measure expression levels of 20,000 genes. So mathematically, this is 20,000 dimensional vector. And then also, let's say you consider a pair of drugs, drugs A and B, and then you use various information about this drug. For example, structure of these drugs or their biological targets. There are many data sets that can tell you that information. And then you take those as a machine learning input, and then you want to predict the synergy between the drugs A and B. So in this kind of a problem, and as I said, there will be tens of thousands of pairwise combinations of those drugs. And so in this kind of situation, not only the prediction, but also explanations will be extremely important. So say you want to be able to say that drug A and B is going to work well, are going to have a synergy together because this patient X has gene expression levels of A, B, and C high. Or you say expression levels of a certain biological pathway, those genes are highly expressed. So you need a set of explanation to do that. And then more importantly, if you think about all pairs of drugs. If there is an underlying principle in terms of when two drugs are likely to have a synergy, then it's going to be even more useful. So what we did in this paper was that we got the explanations. We computed the shaft values for many combinations of drugs from the machine learning model, and then we analyzed that, and then we identified the unifying principle in terms of when, in what case, any pair of drugs A and B have a synergy, and then we identified the pathway. It is called stemness pathway. It is also called, trying to find in that part of the slide, this hematopoietic stem cell-like signature. Cancers are sometimes more differentiated or less differentiated. If you had a family member who had cancer, you probably understand this term. Usually, less differentiated cancers have worse prognosis than more differentiated cancers. we identified this pathway that's really relevant to this stem-ness mechanism and then found the underlying principle, which basically says that it's good to have two drugs, one drug targeting less differentiated, the other one targeting more differentiated cancer, likely work the best. So in this project, not only our algorithm can tell oncologists or biological scientists which genes are important, which feature attributions, which features are important for drug synergy. But also, by analyzing many model explanations from many patients, we can have an understanding of these underlying principles in terms of what makes a successful drug combination therapy. Cancer therapy design, I would say. This is an example where we can see how explainable AI can be effective in cancer therapy design. Is AML unique in having a well-understood pathway or is that a bottleneck for the application of this technique to the broader set of cancers? Oh, so AML is just one example. I mean, this kind of principle can be applied to too many data sets. You know, computational biologists often need to work on the problem where the data are available. So, you know, as you can imagine, blood cancers, those tissues are relatively easy to, it's relatively easier to obtain, you know, blood tissues compared to other kinds of tissues. There are many available data sets and then also the measurement of the drug synergy from many samples. So we happen to choose this cancer type because of the data availability. But this approach can be broadly applicable to other types of cancer. So this is one of the... Yeah, go ahead. I'm maybe trying to get a broader question, which is the explainability method is kind of explaining over a set of known features and pathways and processes and things like that. And my sense is that for many of the potential applications, the pathways are still a subject of research themselves. Meaning, you know, maybe there's some aspect of pathway that's known, but there are others. There are, you know, or some diseases for which there aren't pathways. And I guess I'm wondering the way you think about applying techniques like this in a... A, is that actually the case or am I all wrong there? But otherwise, how will you apply techniques like this in rapidly evolving fields that are very complex, meaning... That's an excellent question. Maybe you're giving an explanation and the explanation is based on the pathway as you understand it, but there's so many other things going on in the system that you really have not accounted for. Yeah, exactly. Right. So first of all, Pathway is not unique to disease. So when we say, you know, Pathway databases, it basically tells you the members of the genes in each pathway. That's it. I mean, it's like, you know, many, many sets of genes. We also sometimes call it gene sets. It doesn't depend on the disease. And then the way we view is that it's not like all genes need to be activated for the pathway needs to be activated. It would be only a subset of genes. We would expect only a subset of genes to be highly expressed to say, you know, that pathway is activated. And then it's really extremely important for a computational biologist when we develop, you know, a method like this to get biological insights from large scale data sets. When we develop such a method, we need to make sure that it does not fully depend on any sort of prior knowledge. And then the algorithm needs to be flexible. So that's of key importance. So in this particular example, we didn't use a pathway actually from the beginning. When the model training happens, we used genes as individual features. And then we analyzed the feature attributions and then did the statistical test to see which pathways seem to be more activated. You made a really good point. In all computational biology methods, it's really important not to make it too rigid. For the existing knowledge, it needs to be flexible. And so how do you evaluate your results in this particular paper? Oh, so say that you have a feature attribution for all genes, for a certain patient, and then for a certain combination of drugs. And then say you will have a lot of feature attributions then, right? Combining all patients and all pairs of drugs you considered. And then we perform the statistical test. So for example, it's a simple, you know, features exact test kind of statistical test where you see whether there is significantly, you know, large value of attribution values for certain set of genes defined by certain pathway. And then you do, you know, multiple hypothesis testing and then see whether that, you know, significance is indeed is relevant. So the pathway-based analysis was done in a post-hoc manner after model training and then obtaining all, you know, model explanations. So another challenge we ran into in that project was that was really not addressed properly by this foundational AI field was feature correlation. So in many biomedical data sets, you will see lots of features that are correlated with each other. Many genes are correlated. It's a really modular gene expression, you know, levels are very modular, so you easily see a subset of genes that are very highly correlated with each other. So in that kind of case, shaft values are not going to be extremely accurate because imagine that there are two genes that are perfectly correlated with each other. Then There will be infinite ways to attribute to these two genes. So in that paper, in that Nature Biomedical Engineering paper, we addressed it by considering ensemble model. So we ran many ensemble of model explanations. So we ran the model. In this case, it was not your deep neural network. It was three ensembles. And then we averaged. We averaged the feature attributions that are from many models. And then we showed that it gives you more robust feature attributions when the features are correlated with each other. Awesome. So talk a little bit about where you see the future of your research going. That's a really important question. In all three ways, first of all, in the foundational AI method, as I briefly mentioned, this robustness issues and also multi-modal data. Let's say that you have a set of features and each feature belongs to different category. They are in different modality and then how to attribute to these features that are in different modalities. So that's an open problem. So it was actually motivated by biomedical problem, but it's widely applicable to other applications. And then also these emerging models of LLMs or other foundational models. And in this kind of really large models, how to actually compute the feature attributions properly. And then also, we are really interested in sample-based importance to say that you transpose the matrix transpose of your feature matrix. So I've been talking about these feature attributions a lot, but you can also apply Shapley values to gain insights into which samples are important for your model training. So that can help us understand how foundational models in various fields or large language models rely on which training samples. So that can be really important for model auditing perspective, first of all, and then to gain insight in terms of which samples were important. for these large models to behave a certain way, right? So sample-based explanation is also one of the things that we are mainly working on. In the biomedical side, there are many projects. So, you know, single cell data science is one of the big themes in my lab now. So you obtain gene expression levels or other kinds of molecular level information at a single cell level. So the advantage is that you will have a ton of samples. So one experiment is going to give you many samples, which is really appropriate for large scale models these days based on deep neural networks. So for example, the researchers started looking into foundational model for a single cell data set. So in this kind of, you know, data sets that have still, you know, high dimensional and then researchers are now obtaining multi-omic data. So not only gene expressions, you can also obtain other kinds of, you know, genomic information. So that's going to increase the dimensionality also, and then larger sample sizes. How to learn the biologically interpretable representation space? That's one of the big questions in the research in my lab. All feature attribution methods at the end in the downstream prediction test, you attribute to features. And then the assumption is that each feature is an interpretable unit. In biology, as I mentioned earlier, it's not the case in biology, right? So the functional units in biology is much more interpretable than in any individual genes. So how to learn the features that have more broadly representation, feature representation space that's biologically more interpretable. And then also how to make foundational models learned based on single cell data sets. So researchers started publishing those papers that are about applying this foundational model approach to single cell data sets. And then how to make it biologically interpretable so that you can gain scientific insights from the model results and then also audit those models to make sure that users can actually safely use them for scientific discoveries. So, attribution methods for this kind of modern machine learning models so that you can gain biological insights. So that's another theme. In a clinical side, we are really interested in this model auditing. In our most recent paper that's in review, we are focusing on dermatology example. So dermatological image is inputted into deep neural network, and then you want to know whether the prediction result is melanoma or not. There are many algorithms out there, some published in very high-profile medical journals, and also some available through the cell phone apps. So there are many algorithms. And then we recently tested them, we just separate the held out tested samples, and then got the result that's a little concerning in terms of usage. And then our analysis showed that explainable AI was extremely helpful. So for example, in the skin image, which part of the image led to that kind of a prediction? Or as I said, using this counterfactual image generation. So you make small changes to the input dermatology image such that it changes. It crosses the decision boundary of the classifier and then see what features were changes. So that way you can see the reasoning process of this classifier. the clinical AI model. So for that, there needs to be some technological development there because the feature attributions themselves are not going to be enough. It shows only very small part of the inner workings of the machine learning model. So developing methods for auditing clinical AI models, that's the research we are currently performing in the clinical area. So all three areas, we are doing exciting research. Well, Suwan, it sounds like you've got a lot of work ahead of you. Yes. Yeah. Very busy. I bet. Thanks so much for joining us. Thank you. Thank you for inviting me. Thank you. All right, everyone, that's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit TwiMLAI.com. Of course, if you like what you hear on the podcast, please subscribe, rate, and review the show on your favorite podcatcher. Thanks so much for listening and catch you next time."
    },
    "podcast_summary": "In this podcast, Suin Lee, a professor at the University of Washington, discusses her research on explainable AI (XAI) in the field of computational biology and clinical medicine. She explains how her lab is working on developing XAI techniques for understanding the causes and treatments of diseases like cancer and Alzheimer's, as well as for clinical diagnosis and auditing frameworks. Suin also highlights the importance of collaboration between computer science, biology, and medicine to advance these fields and improve patient outcomes. She emphasizes the need for explainable AI methods that go beyond feature-level explanations and provide system-level insights. One of the applications she discusses is the use of XAI in cancer therapy design, where her team identified a pathway that is relevant to drug synergy and demonstrated how XAI can uncover underlying principles in treatment efficacy. Suin concludes by mentioning future research directions, such as addressing robustness and multi-modality issues in foundational AI methods and applying XAI to single-cell data analysis and model auditing in clinical settings.",
    "podcast_guest": {
        "name": "Suin Lee",
        "job": "professor",
        "summary": "",
        "URL": "https://aims.cs.washington.edu/su-in-lee"
    },
    "podcast_highlights": "- Highlight 1 of the podcast: \"If machine learning researchers develop a powerful and effective method to identify cause of diseases such as cancer and then therapeutic targets, for those diseases, I can contribute hugely to the science and also medicine.\"\n- Highlight 2 of the podcast: \"To actually move the field forward, to advance this field of biology, medicine, or biomedical sciences, you really need to become a bilingual researcher, or even trilingual these days.\"\n- Highlight 3 of the podcast: \"Imagine that you have a deep model, deep neural network, that's going to take you a dermatology image... And then let's say that you want to know that has features of melanoma or not. So the prediction results itself is not going to be really useful.\"\n- Highlight 4 of the podcast: \"We identified a pathway that's really relevant to this stem-ness mechanism and then found the underlying principle, which basically says that it's good to have two drugs, one drug targeting less differentiated, the other one targeting more differentiated cancer, likely work the best.\"\n- Highlight 5 of the podcast: \"How to make foundational models learned based on single cell data sets... So how to learn the features that have more broadly representation, feature representation space that's"
}